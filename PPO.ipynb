{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM05rezxAUiOkHqJkbva+3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newmantic/PPO/blob/main/PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s7j4nHfGyGqe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=64):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # Actor network\n",
        "        self.actor = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "        # Critic network\n",
        "        self.critic = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        policy_logits = self.actor(x)\n",
        "        state_value = self.critic(x)\n",
        "        return policy_logits, state_value\n",
        "\n",
        "    def act(self, state):\n",
        "        policy_logits, _ = self.forward(state)\n",
        "        dist = Categorical(logits=policy_logits)\n",
        "        action = dist.sample()\n",
        "        action_log_prob = dist.log_prob(action)\n",
        "        return action.item(), action_log_prob\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        policy_logits, state_value = self.forward(state)\n",
        "        dist = Categorical(logits=policy_logits)\n",
        "        action_log_probs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        return action_log_probs, torch.squeeze(state_value), dist_entropy"
      ],
      "metadata": {
        "id": "IAg94WLAySpx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOAgent:\n",
        "    def __init__(self, state_size, action_size, hidden_size=64, lr=0.001, gamma=0.99, clip_epsilon=0.2, update_epochs=10, c1=0.5, c2=0.01):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.clip_epsilon = clip_epsilon\n",
        "        self.update_epochs = update_epochs\n",
        "        self.c1 = c1  # Value function loss coefficient\n",
        "        self.c2 = c2  # Entropy bonus coefficient\n",
        "\n",
        "        self.model = ActorCritic(state_size, action_size, hidden_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "    def compute_returns(self, rewards, dones, next_value):\n",
        "        returns = []\n",
        "        R = next_value\n",
        "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
        "            R = reward + self.gamma * R * (1 - done)\n",
        "            returns.insert(0, R)\n",
        "        return returns\n",
        "\n",
        "    def update(self, states, actions, log_probs, returns, advantages):\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        old_log_probs = torch.FloatTensor(log_probs)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "        advantages = torch.FloatTensor(advantages)\n",
        "\n",
        "        for _ in range(self.update_epochs):\n",
        "            new_log_probs, state_values, entropy = self.model.evaluate(states, actions)\n",
        "            ratios = torch.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            critic_loss = self.c1 * (returns - state_values).pow(2).mean()\n",
        "            entropy_bonus = self.c2 * entropy.mean()\n",
        "\n",
        "            loss = actor_loss + critic_loss - entropy_bonus\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "cjNUndv6yZD_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Simple1DEnv:\n",
        "    def __init__(self, length=10, start=0, goal=9):\n",
        "        self.length = length\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.state = start\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start\n",
        "        return np.array([self.state], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:  # move left\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == 1:  # move right\n",
        "            self.state = min(self.length - 1, self.state + 1)\n",
        "\n",
        "        reward = 1 if self.state == self.goal else -0.1\n",
        "        done = self.state == self.goal\n",
        "        return np.array([self.state], dtype=np.float32), reward, done"
      ],
      "metadata": {
        "id": "Dcb9griVyaDg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ppo():\n",
        "    env = Simple1DEnv()\n",
        "    state_size = 1  # since the state is just the position in the 1D space\n",
        "    action_size = 2  # two possible actions: move left or right\n",
        "\n",
        "    agent = PPOAgent(state_size, action_size, hidden_size=64, lr=0.001, gamma=0.99, clip_epsilon=0.2, update_epochs=10, c1=0.5, c2=0.01)\n",
        "\n",
        "    n_episodes = 500\n",
        "    max_steps = 100\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "        dones = []\n",
        "        actions = []\n",
        "        states = []\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            state_tensor = torch.FloatTensor(state)\n",
        "            action, log_prob = agent.model.act(state_tensor)\n",
        "            _, value = agent.model(state_tensor)\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            values.append(value)\n",
        "            dones.append(done)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done or step == max_steps - 1:\n",
        "                next_state_tensor = torch.FloatTensor(next_state)\n",
        "                _, next_value = agent.model(next_state_tensor)\n",
        "                next_value = next_value.item()\n",
        "\n",
        "                returns = agent.compute_returns(rewards, dones, next_value)\n",
        "                advantages = [ret - val.item() for ret, val in zip(returns, values)]\n",
        "                agent.update(states, actions, log_probs, returns, advantages)\n",
        "\n",
        "                if episode % 10 == 0:\n",
        "                    print(f\"Episode {episode+1}/{n_episodes}, Total Reward: {sum(rewards)}\")\n",
        "                break\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "# Train the PPO agent\n",
        "ppo_agent = train_ppo()\n",
        "\n",
        "# Save the trained model\n",
        "ppo_agent.save(\"ppo_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulrIQz7NydI3",
        "outputId": "ec6f052e-acbc-432c-90fe-06cc60d84910"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/500, Total Reward: -0.7000000000000004\n",
            "Episode 11/500, Total Reward: 0.20000000000000007\n",
            "Episode 21/500, Total Reward: 0.20000000000000007\n",
            "Episode 31/500, Total Reward: 1.1102230246251565e-16\n",
            "Episode 41/500, Total Reward: 0.20000000000000007\n",
            "Episode 51/500, Total Reward: 0.20000000000000007\n",
            "Episode 61/500, Total Reward: 0.20000000000000007\n",
            "Episode 71/500, Total Reward: 0.20000000000000007\n",
            "Episode 81/500, Total Reward: 0.20000000000000007\n",
            "Episode 91/500, Total Reward: 0.20000000000000007\n",
            "Episode 101/500, Total Reward: 0.20000000000000007\n",
            "Episode 111/500, Total Reward: 0.20000000000000007\n",
            "Episode 121/500, Total Reward: 0.20000000000000007\n",
            "Episode 131/500, Total Reward: 0.20000000000000007\n",
            "Episode 141/500, Total Reward: 0.20000000000000007\n",
            "Episode 151/500, Total Reward: 0.20000000000000007\n",
            "Episode 161/500, Total Reward: 0.20000000000000007\n",
            "Episode 171/500, Total Reward: 0.20000000000000007\n",
            "Episode 181/500, Total Reward: 0.20000000000000007\n",
            "Episode 191/500, Total Reward: 0.20000000000000007\n",
            "Episode 201/500, Total Reward: 0.20000000000000007\n",
            "Episode 211/500, Total Reward: 0.20000000000000007\n",
            "Episode 221/500, Total Reward: 0.20000000000000007\n",
            "Episode 231/500, Total Reward: 0.20000000000000007\n",
            "Episode 241/500, Total Reward: 0.20000000000000007\n",
            "Episode 251/500, Total Reward: 0.20000000000000007\n",
            "Episode 261/500, Total Reward: 0.20000000000000007\n",
            "Episode 271/500, Total Reward: 0.20000000000000007\n",
            "Episode 281/500, Total Reward: 0.20000000000000007\n",
            "Episode 291/500, Total Reward: 0.20000000000000007\n",
            "Episode 301/500, Total Reward: 0.20000000000000007\n",
            "Episode 311/500, Total Reward: 0.20000000000000007\n",
            "Episode 321/500, Total Reward: 0.20000000000000007\n",
            "Episode 331/500, Total Reward: 0.20000000000000007\n",
            "Episode 341/500, Total Reward: 0.20000000000000007\n",
            "Episode 351/500, Total Reward: 0.20000000000000007\n",
            "Episode 361/500, Total Reward: 0.20000000000000007\n",
            "Episode 371/500, Total Reward: 0.20000000000000007\n",
            "Episode 381/500, Total Reward: 0.20000000000000007\n",
            "Episode 391/500, Total Reward: 0.20000000000000007\n",
            "Episode 401/500, Total Reward: 0.20000000000000007\n",
            "Episode 411/500, Total Reward: 0.20000000000000007\n",
            "Episode 421/500, Total Reward: 0.20000000000000007\n",
            "Episode 431/500, Total Reward: 0.20000000000000007\n",
            "Episode 441/500, Total Reward: 0.20000000000000007\n",
            "Episode 451/500, Total Reward: 0.20000000000000007\n",
            "Episode 461/500, Total Reward: 0.20000000000000007\n",
            "Episode 471/500, Total Reward: 0.20000000000000007\n",
            "Episode 481/500, Total Reward: 0.20000000000000007\n",
            "Episode 491/500, Total Reward: 0.20000000000000007\n"
          ]
        }
      ]
    }
  ]
}